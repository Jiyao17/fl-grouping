
config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 50, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.SGD: 1>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}

config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 50, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.SGD: 1>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}

config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 50, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.SGD: 1>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 50, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.FEDCLAR: 4>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 50, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.FEDCLAR: 4>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 50, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.FEDCLAR: 4>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.9088128306878307 0.9579124579124579 0.9711550024050024 0.9830447330447329 0.9510582010582009 0.9678932178932179 0.9648989898989898 0.9711550024050024 0.98989898989899 0.959606180856181 0.9780092592592594 0.9560034872534873 0.9611592111592112 0.9712602212602214 0.9613443963443964 0.9511784511784511 0.9480134680134681 0.962857744107744 0.9578072390572391 0.9763279048993335 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 60, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.FEDCLAR: 4>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.9238752319109462 0.9390647126287774 0.9083848159694993 0.9730639730639731 0.9202054559197416 0.9542297979797979 0.9690699733501648 0.9760637840994985 0.9694339225589226 0.9593554593554593 0.9828504002611145 0.9728009259259259 0.8848449853751769 0.8995686580369938 0.9454494140153198 0.9645949288806431 0.9745739178728869 0.98621632996633 0.9480292723149866 0.9266753418539132 0.9471865987882188 0.984674927973897 0.9558617638974781 0.9654914586109357 0.977747285783 0.9214133254311826 0.9796777296777298 0.9710487098879955 0.8734573627430771 0.914910908651704 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 60, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.FEDCLAR: 4>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.9225589225589225 0.9123204837490552 0.9255788121767504 0.9524811903309695 0.9447644177946093 0.98292306613367 0.961877147766323 0.9021149853255892 0.9661238232666803 0.9626852719636224 0.8979377104377105 0.9619129438717068 0.9494949494949495 0.9292929292929294 0.9689625850340136 0.9426578712292998 0.9559870391976429 0.9291898577612864 0.9589646464646465 0.9561258846973132 0.983026137665313 0.9258228543942829 0.9118265993265994 0.9694550526656563 0.9661224064758676 0.9325568611282898 0.9493561039952793 0.9291898577612864 0.9689625850340136 0.9459891006282759 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
config:{'task_name': <TaskName.CIFAR: 1>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 10, 'FedCLAR_tl_epoch': 20, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.FEDCLAR: 4>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0.8158970658970658 0.7925857211571498 0.7962373223787068 0.936026936026936 0.8371914318158796 0.8148939780014891 0.7772795987081701 0.8339267915851126 0.7955232598089741 0.7724008795437366 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 
config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 10, 'FedCLAR_tl_epoch': 20, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.FEDCLAR: 4>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0.9265259368352151 0.9190888476602762 0.9373320837238364 0.9472248255753412 0.9126314693325004 0.9553494124922697 0.927064459023222 0.9878375592661307 0.9531621708352194 0.9575345289631004 0.8904732152154832 0.9609501282549884 0.9672615062894886 0.8969696969696971 0.9148182859523066 0.7997320140177283 0.8906411049268191 0.942601270436322 0.9367089791242957 0.9212121212121213 0.9170480313337457 0.8925548351174273 0.9693220868478601 0.9635951350237064 0.9386858273456212 0.9140092275585647 0.9654482974070604 0.8660014833799812 0.914652096389946 0.8663334374674581 0.9474747474747474 0.9108616142489485 0.862251379777153 0.8861631250438318 0.8881391231906696 0.8967202001075345 0.9141350385827558 0.959472273757988 0.8585858585858585 0.9757575757575758 0.9568259918775383 0.882370092679371 0.9696969696969697 0.8967429396000824 0.9367944116103175 0.8924298741677239 0.9183171925439966 0.9266908512858439 0.9652833829564316 0.971383942517963 0.8988249845392703 0.8559825054670416 0.9345663663778538 0.9548491436562129 0.9609509783294762 0.9670519629282517 0.8705833848690991 0.9509736540664377 0.9531188170363428 0.8880145872782104 0.927064459023222 0.8825809111523398 0.8658155295857799 0.9026970738310943 0.8987816307403935 0.9737373737373737 0.9088229231086373 0.9492866812454441 0.8725005153576582 0.961241703804296 0.8564007421150277 0.8946608946608947 0.8493804019577216 0.8787466501752217 0.9059069550969404 0.9512265512265511 0.9527439341872332 0.9691554722482557 0.9228574403832136 0.9572025748756234 0.9390228818800248 0.9836734693877552 0.9468499427262314 0.9261725183669218 0.9472479901051329 0.888556934801412 0.8766027623170481 0.9229824013329168 0.8867450010307154 0.946665264043762 0.8905174190888477 0.967408781694496 0.9130076272933415 0.9412904555761699 0.9711340206185568 0.96134817563389 0.9512471655328799 0.9468739573305112 0.953308596165739 0.9608247422680412 0.8723356009070293 0.9691138185983548 0.938728756107254 0.975382692908466 0.8945746121003854 0.9652829579191877 0.9592455163883736 0.8945990517419089 0.9387691346454234 0.9150278293135437 0.8516535011380372 0.9208788920129125 0.9151515151515153 0.9353535353535355 0.8054774549619911 
config:{'task_name': <TaskName.FMNIST: 3>, 'server_num': 1, 'client_num': 100, 'data_num_range': (10, 51), 'alpha': (0.05, 0.05), 'sampling_frac': 0.2, 'data_path': './data/', 'budget': 10000000, 'global_epoch_num': 1000, 'FedCLAR_cluster_epoch': 30, 'FedCLAR_tl_epoch': 1000, 'FedCLAR_th': 0.1, 'group_epoch_num': 5, 'local_epoch_num': 2, 'lr_interval': 1000, 'lr': 0.001, 'batch_size': 5, 'device': 'cuda', 'train_method': <TrainMethod.FEDCLAR: 4>, 'selection_mode': <SelectionMode.RANDOM: 10>, 'aggregation_option': <AggregationOption.WEIGHTED_AVERAGE: 1>, 'grouping_mode': <GroupingMode.RANDOM: 2>, 'max_group_cv': 1.0, 'min_group_size': 5, 'log_interval': 1, 'result_dir': './exp_data/grouping/fedclar/', 'test_mark': '_fmnist', 'comment': ''}
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.9577739984882842 0.9254610733182161 0.9236573108885334 0.84566687280973 0.9306843949701094 0.8778598808053889 0.9066168381337748 0.88809728889994 0.9442723043606697 0.969645079733445 0.850786779358208 0.9187964574562512 0.8864319965203618 0.9762760149358088 0.9185391328248471 0.8865347351061637 0.9427262313860253 0.9478279392565107 0.9308726723012438 0.9459729846327786 0.9456122342720281 0.8863808149522434 0.9407682264825122 0.8964131106988248 0.9712420013303665 0.9629629629629629 0.9578258778258778 0.9272658558372845 0.9544588744588743 0.9680299594585309 0.8862605648319936 0.879663986806844 0.8914306328592043 0.9203944203944204 0.9052593966879682 0.9322469593898166 0.9000206143063285 0.9355802927231499 0.9764646464646464 0.9796949082663368 0.9051226551226551 0.9389981447124304 0.957946127946128 0.9697306397306397 0.928829107400536 0.9763443963443964 0.9323335394763966 0.9528956228956229 0.8946952518381089 0.9579124579124579 0.9152408438122723 0.9202226345083488 0.9746444032158318 0.9237779152064868 0.9528619528619529 0.9595959595959597 0.9475015460729747 0.9491685563114135 0.9322304679447537 0.9117013674156532 0.9458352229780802 0.9662268947983234 0.9730976430976431 0.9662605648319933 0.9426915412629698 0.9474850546279119 0.968013468013468 0.9714141414141414 0.895055315055315 0.9662605648319934 0.9355974713117571 0.9494423830138116 0.9696969696969696 0.9576197347625918 0.9187280972995259 0.8913454270597129 0.9729945715660002 0.9831986531986532 0.9508864151721294 0.98993265993266 0.9729945715660001 0.9404761904761904 0.9713973063973064 0.9373840445269016 0.9355631141345427 0.8779633065347351 0.9237270665842096 0.9086435786435786 0.9764646464646466 0.9525699168556313 0.8965154950869237 0.942296433725005 0.9273338830481688 0.9510753796468082 0.8744932316360888 0.9545454545454545 0.9662605648319934 0.905105476534048 0.9271971414828557 0.946127946127946 0.9356483199340343 0.9679103964818251 0.9426578712292998 0.9508692365835222 0.9441352298495156 0.9712773998488283 0.9153267367553082 0.9612794612794614 0.9184013605442177 0.9204803133374563 0.9730639730639731 0.9612794612794614 0.9288291074005359 0.879680821823679 0.9628598914313199 0.9238301381158526 0.9764309764309763 0.9388957603243316 0.9255995327423898 0.9354428640142927 0.9135741771456057 0.9340342197485055 0.9645433931148216 0.9306668728097299 0.9763279048993335 0.9746444032158318 0.9119081976224833 0.95782622139765 0.9609702466845325 0.9031127602556174 0.9646464646464646 0.9388957603243316 0.9477252112966399 0.9474850546279118 0.9646801346801347 0.957809386380815 0.9764646464646466 0.9474850546279118 0.9219748505462791 0.9612794612794612 0.9613131313131312 0.9234693877551021 0.9287775716347145 0.922060743489315 0.8879784236927094 0.9389816532673675 0.9814814814814814 0.9068748711605855 0.9612794612794614 0.9949494949494949 0.947708376279805 0.9492029134886278 0.9645433931148218 0.8830306466020752 0.9528787878787878 0.8812959527245242 0.9831818181818183 0.9595959595959597 0.9763447399161684 0.9390675462104033 0.9611763897478184 0.9015148079433795 0.9220947570947572 0.9357005428433999 0.9015495086923658 0.8946780732495019 0.9544423830138116 0.9270768913626056 0.9697138047138049 0.9290005497148354 0.9339139696282553 0.9713804713804713 0.9136088778945921 0.962996632996633 0.8880471380471381 0.9390675462104033 0.9441517212945785 0.9202398130969559 0.9491685563114135 0.9170095512952656 0.9388957603243316 0.9596127946127946 0.9679272314986601 0.9204459561602419 0.9475022332165189 0.9253933896791039 0.9713804713804713 0.9320415034700749 0.8846973132687418 0.9696110767539339 0.9645774067202639 0.9289665361093932 0.9491685563114135 0.9354943997801141 0.9813952449666735 0.957929292929293 0.9713973063973064 0.9611932247646534 0.9355967841682128 0.9360437710437709 0.9680303030303031 0.968013468013468 0.8947639661925376 0.9542190613619185 0.9049161684875972 0.9592864014292585 0.9253243317529032 0.946127946127946 0.9578093863808149 0.920376898234041 0.9561258846973133 0.9882154882154882 0.9561258846973133 0.8883735312306742 0.9511784511784511 0.9357005428433999 0.9081632653061225 0.9882154882154882 0.9645433931148218 0.9545622895622895 0.962979797979798 0.9271109049680478 0.9441352298495156 0.9441180512609084 0.9628598914313199 0.9610214388785817 0.9458015529444102 0.9474850546279118 0.9304779083350513 0.9034391534391534 0.9493918779633065 0.8812100597814884 0.9511784511784512 0.937195080052223 0.9169930598502027 0.9220603999175427 0.9338280766852195 0.9444444444444445 0.9254277468563182 0.9338452552738267 0.9595959595959597 0.9047619047619048 0.9781313131313132 0.9015663437092009 0.9304610733182161 0.911650518793376 0.9713973063973064 0.937195080052223 0.900175221603793 0.9424517281660139 0.9423314780457638 0.9068745275888133 0.9035250463821892 0.9220435649007078 0.9697138047138049 0.9712773998488284 0.9662268947983234 0.942262420119563 0.9321445750017179 0.9237270665842093 0.9508688930117501 0.9797979797979798 0.9865319865319865 0.9746444032158319 0.9236239950525665 0.9185559678416823 0.9338280766852196 0.9596127946127946 0.9797120868549439 0.9797979797979798 0.9239503882361025 0.9354943997801141 0.9119593898165328 0.9424517281660139 0.9357177214320073 0.9220435649007078 0.9427609427609428 0.9612962962962963 0.9018243661100804 0.9406651549508692 0.9747811447811449 0.9001748780320208 0.9441180512609084 0.9646632996632997 0.9562289562289562 0.9306668728097299 0.9798148148148149 0.9579124579124579 0.9562457912457912 0.9253933896791039 0.9713117570260428 0.9084896584896586 0.9153432282003712 0.9357005428434001 0.9305641448498592 0.9135229849515564 0.9427609427609428 0.9101899951899952 0.9254105682677111 0.9932659932659932 0.9323503744932317 0.9205490276918847 0.9612962962962963 0.9169930598502027 0.9203768982340411 0.98989898989899 0.9814814814814815 0.9303576582148011 0.8947811447811448 0.9081632653061225 0.9696969696969697 0.9321442314299458 0.9593039235896379 0.9697138047138046 0.9491857349000207 0.9545454545454546 0.8796296296296297 0.9391015598158455 0.9679440665154951 0.974661581804439 0.9596296296296297 0.9270940699512128 0.9169930598502027 0.9220772349343779 0.9116161616161618 0.9256167113309971 0.9323335394763966 0.9645433931148216 0.9239332096474954 0.9323507180650038 0.9303916718202433 0.9032495018209303 0.968013468013468 0.915309558166701 0.9797979797979798 0.8879612451041021 0.9730976430976431 0.947399161684876 0.9663468013468014 0.9494087129801416 0.9152236652236653 0.9473819830962689 0.9848484848484849 0.8862605648319933 0.9763790970933828 0.9545791245791246 0.9697306397306398 0.8930289287432145 0.9562289562289563 0.9511265718408576 0.8829450972308116 0.9474850546279118 0.9254614168899883 0.9391183948326806 0.9562289562289562 0.9662777434206005 0.9237779152064866 0.968013468013468 0.8981653267367552 0.8951075379646808 0.9287775716347145 0.9747474747474746 0.9746780732495018 0.9612272383700954 0.9696447467876039 0.9339139696282553 0.9730976430976431 0.8846629560915275 0.9646464646464646 0.915412629698344 0.9138150209578781 0.937195080052223 0.957843056414485 0.9322304679447537 0.8846629560915275 0.9474678760393047 0.9764646464646464 0.9561767333195905 0.9561430632859205 0.8982161753590324 0.9730639730639731 0.9512121212121212 0.9646801346801347 0.968013468013468 0.9170102384388099 0.9611763897478184 0.932161753590325 0.9764309764309765 0.94580155294441 0.889764996907854 0.9662440733869305 0.9423830138115853 0.974695251838109 0.9372122586408299 0.9135910121624408 0.9357005428433999 0.9713804713804715 0.9460592317735176 0.9372459286745002 0.9561430632859205 0.9015831787260359 0.8796468082182368 0.9747811447811449 0.9578774135916994 0.9320930392358964 0.9340342197485055 0.9377104377104377 0.9798316498316497 0.9830962688105545 0.9119934034219749 0.9237442451728167 0.9444094001236859 0.9830790902219474 0.9405957534528963 0.9018587232872948 0.968013468013468 0.9152745138459425 0.9356833642547929 0.928965848965849 0.9730639730639731 0.9458180443894729 0.9372459286745002 0.937195080052223 0.9525348725348725 0.9628770700199271 0.9048642891500034 0.9494599051741911 0.8981825053253624 0.9696969696969697 0.9508341922627638 0.9542190613619187 0.9611763897478184 0.9629629629629629 0.9441517212945784 0.9082491582491583 0.930443894729609 0.957946127946128 0.9764646464646464 0.9595959595959597 0.9239332096474954 0.9323672095100667 0.938809867381296 0.9255651755651755 0.9256160241874527 0.9612107469250327 0.9696969696969697 0.9323507180650038 0.9052257266542981 0.9015495086923658 0.9475187246615818 0.9512121212121212 0.9221466364323508 0.9202913488627775 0.9288284202569916 0.9408190751047893 0.9405105476534049 0.915309558166701 0.9575173503744931 0.9579461279461281 0.9595959595959597 0.9815151515151515 0.9322304679447537 0.9221466364323506 0.9354765340479627 0.9220944135229848 0.9562289562289562 0.9663299663299663 0.9069428983714699 0.9561767333195904 0.9865319865319866 0.9646464646464646 0.9186937401223115 0.9407503607503607 0.888080808080808 0.9713804713804713 0.9764646464646464 0.9629629629629629 0.9831127602556174 0.9444444444444445 0.9646801346801347 0.969611076753934 0.9323335394763966 0.9048821548821548 0.940784717927575 0.9321095306809593 0.9422627636913351 0.9444781144781146 0.9613131313131315 0.933793032364461 0.9321782450353879 0.8897986669415241 0.9595959595959597 0.9508692365835224 0.9443922215350787 0.9304782519068233 0.9528619528619529 0.9444444444444445 0.928759705902563 0.9612444169587026 0.9393939393939394 0.9528619528619529 0.9152408438122723 0.9121308321308321 0.9069092283377999 0.9135566549852264 0.9238301381158524 0.9441860784717928 0.9713804713804713 0.962996632996633 0.9305813234384663 0.9646464646464646 0.9815151515151515 0.9188483474197761 0.9355115783687212 0.9764309764309763 0.9932659932659932 0.9237270665842093 0.9595437366865939 0.9236583522297809 0.9254442383013811 0.9254442383013813 0.9478114478114478 0.9524840239125952 0.9339311482168625 0.9578437435580293 0.9203078403078404 0.9238129595272451 0.9186593829450972 0.9220944135229848 0.9204109118394833 0.9237270665842093 0.9595959595959597 0.9136088778945922 0.9645942417370988 0.9478451178451177 0.9814814814814815 0.8863973063973064 0.9527932385075242 0.959527245241531 0.9306493506493506 0.9338967910396482 0.9085755514326944 0.9390675462104033 0.9287088572802858 0.9780794337937194 0.898491032776747 0.9153954511097369 0.9931972789115647 0.9595959595959597 0.9528619528619529 0.937195080052223 0.9303751803751803 0.9865656565656566 0.9047619047619048 0.9713804713804713 0.9494599051741911 0.9237435580292722 0.9713804713804715 0.9729780801209373 0.9355802927231499 0.9270940699512128 0.9561602418745275 0.9764309764309763 0.9477427334570191 0.9102755445612587 0.9082491582491583 0.9253418539132824 0.968047138047138 0.9595959595959597 0.8897299525870954 0.9477427334570191 0.9781481481481481 0.9830962688105546 0.98989898989899 0.9831649831649831 0.9646801346801347 